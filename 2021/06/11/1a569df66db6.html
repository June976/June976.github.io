<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/favicon/favicon-16x16.png?v=2.6.2" type="image/png" sizes="16x16"><link rel="icon" href="/images/favicon/favicon-32x32.png?v=2.6.2" type="image/png" sizes="32x32"><link rel="apple-touch-icon" href="/images/favicon/apple-touch-icon.png?v=2.6.2" sizes="180x180"><link rel="mask-icon" href="/images/favicon/safari_pinned_tab.svg?v=2.6.2" color="#54bcff"><meta name="msapplication-TileImage" content="/images/favicon/mstile-150x150.png"><meta name="msapplication-TileColor" content="#000000"><meta name="google-site-verification" content="zjHtMN5bFl4jkMXnYa-Qxnv5aCHi4hiWmq19dRvW9dE"><meta name="msvalidate.01" content="3FCE749E09FA739F3F8966641CE2F104"><meta name="baidu-site-verification" content="code-qK1Gt1cc5G"><meta name="description" content="一、环境搭建        使用到的环境：  python3.8.0 requests库 re库 bs4库 pycharm">
<meta property="og:type" content="article">
<meta property="og:title" content="python3爬虫爬取当当网商品信息">
<meta property="og:url" content="https://june976.github.io/2021/06/11/1a569df66db6.html">
<meta property="og:site_name" content="Jun&#39;s Blog">
<meta property="og:description" content="一、环境搭建        使用到的环境：  python3.8.0 requests库 re库 bs4库 pycharm">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019112822563731.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2019112822565831.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20191128225710995.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2021-06-11T06:05:33.000Z">
<meta property="article:modified_time" content="2022-06-08T03:52:25.270Z">
<meta property="article:author" content="Jun">
<meta property="article:tag" content="python3">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/2019112822563731.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70"><title>python3爬虫爬取当当网商品信息 | Jun's Blog</title><link ref="canonical" href="https://june976.github.io/2021/06/11/1a569df66db6.html"><link rel="alternate" href="/sitemap.xml" type="application/atom+xml"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.2"><link rel="dns-prefetch" href="https://hm.baidu.com"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement('script');
  hm.src = 'https://hm.baidu.com/hm.js?eb65d34678c3ed9f0d7fada1c3991648';
  hm.async = true;

  if (false) {
    hm.setAttribute('data-pjax', '');
  }
  var s = document.getElementsByTagName('script')[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":8},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"carbon","highlight":"light","wordWrap":false},
  reward: true,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: false,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"Copy","copySuccess":"Copy Success","copyError":"Copy Error"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 5.4.1"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">Home</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">Archives</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">Tags</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/about/"><span class="header-nav-menu-item__icon"><i class="fas fa-user"></i></span><span class="header-nav-menu-item__text">About</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-sync"></i></span><span class="header-nav-menu-item__text">Next</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/Next/"><span class="header-nav-submenu-item__icon"><i class="fas fa-file-alt"></i></span><span class="header-nav-submenu-item__text">Doc</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="https://next.jun997.xyz/"><span class="header-nav-submenu-item__icon"><i class="fas fa-external-link-alt"></i></span><span class="header-nav-submenu-item__text">To</span></a></div></div></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Jun's Blog</div><div class="header-banner-info__subtitle">Share knowledges, Tell stories and Show something interesting!</div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content" id="content"><!-- Just used to judge whether it is an article page--><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-title">python3爬虫爬取当当网商品信息</h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">Created</span><span class="post-meta-item__value">2021-06-11</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">Updated</span><span class="post-meta-item__value">2022-06-08</span></span><span class="post-meta-item post-meta-item--visitors"><span class="post-meta-item__icon" data-popover="Visitors" data-popover-pos="up"><i class="fas fa-eye"></i></span><span class="post-meta-item__value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body">
        <h1 id="一-环境搭建"   >
          <a href="#一-环境搭建" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#一-环境搭建"></a> 一、环境搭建</h1>
      
<p>使用到的环境：</p>
<ul>
<li>python3.8.0</li>
<li>requests库</li>
<li>re库</li>
<li>bs4库</li>
<li>pycharm</li>
</ul>
<span id="more"></span>

        <h1 id="二-简介"   >
          <a href="#二-简介" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#二-简介"></a> 二、简介</h1>
      
<p>代码实现了根据设定的关键字keyword获取相关商品的资源定位符(url)，然后批量爬取相关页面的商品信息，另外之所以选择当当网是因为当当网的网页商品信息不是动态加载的，因此可以直接爬取获得，例如京东、拼多多的网页就是动态加载的，博主暂时还不会解析动态加载的页面😅😅😅</p>

        <h1 id="三-当当网网页分析"   >
          <a href="#三-当当网网页分析" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#三-当当网网页分析"></a> 三、当当网网页分析</h1>
      

        <h2 id="1-分析网页的url规律"   >
          <a href="#1-分析网页的url规律" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#1-分析网页的url规律"></a> 1、分析网页的url规律</h2>
      
<p>首先是分析出当当网的搜索商品的url，浏览器进入当当网主页，在检索栏输入任意的商品关键词，可以看到打开的页面的url链接形式为如下形式：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&#x27;http://search.dangdang.com/?key=&#123;&#125;&amp;act=input&#x27;</span>.<span class="built_in">format</span>(keyword)     </span><br></pre></td></tr></table></div></figure>
<p>然后获取页面翻页可以发现这类商品的每一页的url形式为：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url = <span class="string">&quot;http://search.dangdang.com/?key=&#123;&#125;&amp;input&amp;page_index=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(keyword,page_count)     </span><br></pre></td></tr></table></div></figure>
<p>分析出了这些规律之后，就可以根据输入的关键词自动生成相应页面的url，然后像服务器发送请求，得到每页的html信息。</p>

        <h2 id="2-解析网页html页面"   >
          <a href="#2-解析网页html页面" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#2-解析网页html页面"></a> 2、解析网页html页面</h2>
      
<p>浏览器打开任一网页，打开网页调试工具可以发现，当当网的商品网页分为两类（很奇葩），第一类是书籍类商品的网页，第二类是其他商品。</p>

        <h4 id="书籍商品html页面解析"   >
          <a href="#书籍商品html页面解析" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#书籍商品html页面解析"></a> 书籍商品html页面解析</h4>
      
<p>经过调试发现，页面的商品信息都在<strong>ul</strong>标签下的中<strong>li</strong>标签中，每个<strong>li</strong>标签块存放一个商品的所有信息，商品的一些信息例如<strong>name</strong>、<strong>price</strong>、<strong>author</strong>等信息又存放在<strong>li</strong>标签下对应的p标签中，因此解析每个p标签的相关属性信息，就能获得对应的信息。值得一提的是书籍商品页面的ul标签的<strong>class</strong>属性是&quot;<strong>bigmig</strong>&quot;。</p>

        <h4 id="其他商品html页面解析"   >
          <a href="#其他商品html页面解析" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#其他商品html页面解析"></a> 其他商品html页面解析</h4>
      
<p>其他商品和书籍类商品的商品信息存放的标签块基本相同，唯一不同的是其<strong>ul</strong>标签的<strong>class</strong>属性是&quot;<strong>bigimg cloth_shoplist</strong>&quot;，解析类似，只不过要区别得到的<strong>ul</strong>标签的<strong>class</strong>属性。</p>

        <h1 id="四-代码实现"   >
          <a href="#四-代码实现" class="heading-link"><i class="fas fa-link"></i></a><a class="markdownIt-Anchor" href="#四-代码实现"></a> 四、代码实现</h1>
      
<p>代码主要包括四个函数：</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getMaxPageCount</span>(<span class="params">keyword</span>):</span></span><br><span class="line">    <span class="comment"># 爬取商品的最大页面数</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        user_agent = &#123;<span class="string">&#x27;user-agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">        max_page_count = <span class="number">0</span></span><br><span class="line">        url = <span class="string">&#x27;http://search.dangdang.com/?key=&#123;&#125;&amp;act=input&#x27;</span>.<span class="built_in">format</span>(keyword)</span><br><span class="line">        rspon = requests.get(url, headers = user_agent)</span><br><span class="line">        rspon.encoding = rspon.apparent_encoding</span><br><span class="line">        rspon.raise_for_status()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取页数失败：&quot;</span>, rspon.status_code)</span><br><span class="line">        <span class="keyword">return</span> max_page_count</span><br><span class="line">    html = BeautifulSoup(rspon.text, <span class="string">&quot;html.parser&quot;</span>)</span><br><span class="line">    ul_tag = html.find(<span class="string">&#x27;ul&#x27;</span>, &#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;Fy&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">for</span> child <span class="keyword">in</span> ul_tag.children:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(child) == <span class="built_in">type</span>(ul_tag):</span><br><span class="line">            match = re.match(<span class="string">r&quot;\d+&quot;</span>,<span class="built_in">str</span>(child.string))</span><br><span class="line">            <span class="keyword">if</span> match:</span><br><span class="line">                temp_num = <span class="built_in">int</span>(match.group(<span class="number">0</span>))</span><br><span class="line">                <span class="keyword">if</span> temp_num &gt; max_page_count:</span><br><span class="line">                    max_page_count = temp_num</span><br><span class="line">    <span class="keyword">return</span> max_page_count</span><br></pre></td></tr></table></div></figure>
<p>getMaxPageCount(keyword) 函数主要是根据输入的商品种类关键词爬取相应的网页，获取当前种类商品的最大页数。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getOnePageMsg</span>(<span class="params">product_list, url</span>):</span></span><br><span class="line">    <span class="comment"># 爬取一个页面的商品数据</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        user_agent = &#123;<span class="string">&#x27;user-agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">        rspon = requests.get(url, headers = user_agent)</span><br><span class="line">        rspon.encoding = rspon.apparent_encoding</span><br><span class="line">        rspon.raise_for_status()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取页数失败：&quot;</span>, rspon.status_code)</span><br><span class="line">    html = BeautifulSoup(rspon.text, <span class="string">&quot;html.parser&quot;</span>)</span><br><span class="line">    ul_tag = html.find(<span class="string">&#x27;ul&#x27;</span>, &#123;<span class="string">&quot;class&quot;</span>: <span class="string">&quot;bigimg cloth_shoplist&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">if</span> ul_tag:</span><br><span class="line">        search_type = <span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ul_tag = html.find(<span class="string">&#x27;ul&#x27;</span>, &#123;<span class="string">&quot;class&quot;</span>: <span class="string">&quot;bigimg&quot;</span>&#125;)</span><br><span class="line">        <span class="keyword">if</span> ul_tag:</span><br><span class="line">            search_type = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">if</span> search_type ==<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> ul_tag.children:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span>(child) == <span class="built_in">type</span>(ul_tag):</span><br><span class="line">                temp_list = []</span><br><span class="line">                <span class="comment"># 保存书名</span></span><br><span class="line">                tag_name = (child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;name&quot;</span>&#125;)).find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_name.attrs[<span class="string">&quot;title&quot;</span>]))</span><br><span class="line">                <span class="comment"># 保存价格</span></span><br><span class="line">                tag_price = (child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;price&quot;</span>&#125;)).find(<span class="string">&quot;span&quot;</span>, &#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;search_now_price&quot;</span>&#125;)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_price.string))</span><br><span class="line">                <span class="comment"># 保存作者</span></span><br><span class="line">                tag_author = child.find(<span class="string">&#x27;p&#x27;</span>, &#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;search_book_author&quot;</span>&#125;).find(<span class="string">&#x27;a&#x27;</span>,&#123;<span class="string">&quot;name&quot;</span>:<span class="string">&quot;itemlist-author&quot;</span>&#125;)</span><br><span class="line">                <span class="keyword">if</span> tag_author:</span><br><span class="line">                    temp_list.append(<span class="built_in">str</span>(tag_author.string))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    temp_list.append(<span class="built_in">str</span>(<span class="string">&quot;NULL&quot;</span>))</span><br><span class="line">                <span class="comment"># 保存出版社</span></span><br><span class="line">                tag_pub = child.find(<span class="string">&#x27;p&#x27;</span>, &#123;<span class="string">&quot;class&quot;</span>: <span class="string">&quot;search_book_author&quot;</span>&#125;).find(<span class="string">&#x27;a&#x27;</span>, &#123;<span class="string">&quot;name&quot;</span>: <span class="string">&quot;P_cbs&quot;</span>&#125;)</span><br><span class="line">                <span class="keyword">if</span> tag_pub:</span><br><span class="line">                    temp_list.append(<span class="built_in">str</span>(tag_pub.string))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    temp_list.append(<span class="built_in">str</span>(<span class="string">&quot;NULL&quot;</span>))</span><br><span class="line">                <span class="comment">#保存评价</span></span><br><span class="line">                tag_comment = child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;search_star_line&quot;</span>&#125;).find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_comment.string))</span><br><span class="line">                product_list.append(temp_list)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> ul_tag.children:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span>(child) == <span class="built_in">type</span>(ul_tag):</span><br><span class="line">                temp_list = []</span><br><span class="line">                <span class="comment"># 保存商品名</span></span><br><span class="line">                tag_name = (child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;name&quot;</span>&#125;)).find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_name.attrs[<span class="string">&quot;title&quot;</span>]))</span><br><span class="line">                <span class="comment"># 保存价格</span></span><br><span class="line">                tag_price = (child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;price&quot;</span>&#125;)).find(<span class="string">&quot;span&quot;</span>)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_price.string))</span><br><span class="line">                <span class="comment">#保存评价</span></span><br><span class="line">                tag_comment = child.find(<span class="string">&#x27;p&#x27;</span>,&#123;<span class="string">&quot;class&quot;</span>:<span class="string">&quot;star&quot;</span>&#125;).find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">                temp_list.append(<span class="built_in">str</span>(tag_comment.string))</span><br><span class="line">                product_list.append(temp_list)</span><br></pre></td></tr></table></div></figure>
<p>getOnePageMsg(product_list, url) 函数实现的是根据一个url爬取网页的html信息，然后解析信息得到需要的商品信息，主要是使用bs4库的BeautifulSoup解析网页结构，并且将数据存入product_list列表中。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getAllPageMsg</span>(<span class="params">keyword, product_list</span>):</span></span><br><span class="line">    <span class="comment"># 爬取商品的所有页面的数据</span></span><br><span class="line">    page_count = getMaxPageCount(keyword)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;find &quot;</span>,page_count,<span class="string">&quot; pages...&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,page_count+<span class="number">1</span>):</span><br><span class="line">        url = <span class="string">&quot;http://search.dangdang.com/?key=&#123;&#125;&amp;input&amp;page_index=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(keyword,i)</span><br><span class="line">        getOnePageMsg(product_list,url)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;&gt;&gt; page&quot;</span>,i,<span class="string">&quot; import successfully...&quot;</span>)</span><br></pre></td></tr></table></div></figure>
<p>getAllPageMsg(keyword, product_list) 函数实现的是根据关键词首先获取这一商品的最大页面，然后自动化生成每一个页面的url，在调用getOnePageMsg函数解析每个url页面，得到数据列表product_list。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeMsgToFile</span>(<span class="params">path,keyword,product_list</span>):</span></span><br><span class="line">    <span class="comment"># 将爬取的数据保存到文件</span></span><br><span class="line">    file_name = <span class="string">&quot;&#123;&#125;.txt&quot;</span>.<span class="built_in">format</span>(keyword)</span><br><span class="line">    file = <span class="built_in">open</span>(path+file_name, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> product_list:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> i:</span><br><span class="line">            file.write(j)</span><br><span class="line">            file.write(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">        file.write(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    file.close()</span><br></pre></td></tr></table></div></figure>
<p>writeMsgToFile(path,keyword,product_list) 函数主要实现将列表的数据写入文件，当然还有根据传入的路径和关键字生成对应的文件名。</p>
<figure class="highlight python"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># 函数入口</span></span><br><span class="line">    keyword = <span class="string">&#x27;python&#x27;</span></span><br><span class="line">    path=<span class="string">&quot;E://&quot;</span></span><br><span class="line">    product_list = []</span><br><span class="line">    getAllPageMsg(keyword,product_list)</span><br><span class="line">    writeMsgToFile(path,keyword,product_list)</span><br></pre></td></tr></table></div></figure>
<p>最后main()函数是实现方法的入口，包括一些参数的初始化设置。<br />
执行main()函数就能实现网页数据的爬取，以下是以python为关键词，爬取页面得到的结果，抓取解析完所有页面花了3分钟左右。<br />
<img   src="https://img-blog.csdnimg.cn/2019112822563731.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70" style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGpx;"  alt="在这里插入图片描述" /><br />
<img   src="https://img-blog.csdnimg.cn/2019112822565831.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70" style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGpx;"  alt="在这里插入图片描述" /><br />
<img   src="https://img-blog.csdnimg.cn/20191128225710995.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGxvV29ybGRUTQ==,size_16,color_FFFFFF,t_70" style="width: image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,tepx;height: t_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0hlbGpx;"  alt="" /><br />
可以看到最终生成的txt文件一共有6000行，每一行保存的就是商品的信息。</p>
</div><footer class="post-footer"><div class="post-ending ending"><div class="ending__text">------ END ------</div></div><div class="post-copyright copyright"><div class="copyright-author"><span class="copyright-author__name">Author: </span><span class="copyright-author__value"><a href="https://june976.github.io">Jun</a></span></div><div class="copyright-link"><span class="copyright-link__name">Link: </span><span class="copyright-link__value"><a href="https://june976.github.io/2021/06/11/1a569df66db6.html">https://june976.github.io/2021/06/11/1a569df66db6.html</a></span></div><div class="copyright-notice"><span class="copyright-notice__name">Copyright: </span><span class="copyright-notice__value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://june976.github.io/tags/python3/">python3</a></span><span class="post-tags-item"><span class="post-tags-item__icon"><i class="fas fa-tag"></i></span><a class="post-tags-item__link" href="https://june976.github.io/tags/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="post-share"><div class="social-share" data-sites="qzone, qq, weibo, wechat, douban, linkedin, facebook, twitter, google">Share to: </div></div><div class="post-reward reward"><div class="reward-button">￥打赏我￥</div><div class="reward-qrcode"><span class="reward-qrcode-alipay"><img class="reward-qrcode-alipay__img" src="/images/PayQRCode/alipay.jpg"><div class="reward-qrcode-alipay__text">Alipay</div></span><span class="reward-qrcode-wechat"><img class="reward-qrcode-wechat__img" src="/images/PayQRCode/wechat.png"><div class="reward-qrcode-wechat__text">Wechat</div></span></div></div><nav class="post-paginator paginator"><div class="paginator-prev"><a class="paginator-prev__link" href="/2021/06/11/41311a1fe8aa.html"><span class="paginator-prev__icon"><i class="fas fa-angle-left"></i></span><span class="paginator-prev__text">计算机网络之ip协议</span></a></div></nav></footer></div></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%80-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">1.</span> <span class="toc-text">
           一、环境搭建</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C-%E7%AE%80%E4%BB%8B"><span class="toc-number">2.</span> <span class="toc-text">
           二、简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%89-%E5%BD%93%E5%BD%93%E7%BD%91%E7%BD%91%E9%A1%B5%E5%88%86%E6%9E%90"><span class="toc-number">3.</span> <span class="toc-text">
           三、当当网网页分析</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%88%86%E6%9E%90%E7%BD%91%E9%A1%B5%E7%9A%84url%E8%A7%84%E5%BE%8B"><span class="toc-number">3.1.</span> <span class="toc-text">
           1、分析网页的url规律</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%A7%A3%E6%9E%90%E7%BD%91%E9%A1%B5html%E9%A1%B5%E9%9D%A2"><span class="toc-number">3.2.</span> <span class="toc-text">
           2、解析网页html页面</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B9%A6%E7%B1%8D%E5%95%86%E5%93%81html%E9%A1%B5%E9%9D%A2%E8%A7%A3%E6%9E%90"><span class="toc-number">3.2.0.1.</span> <span class="toc-text">
           书籍商品html页面解析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E5%95%86%E5%93%81html%E9%A1%B5%E9%9D%A2%E8%A7%A3%E6%9E%90"><span class="toc-number">3.2.0.2.</span> <span class="toc-text">
           其他商品html页面解析</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.</span> <span class="toc-text">
           四、代码实现</span></a></li></ol></section><!-- ov = overview--><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/profile.jpg" alt="avatar"></div><p class="sidebar-ov-author__text">A Journey of A Thousand Miles Begins with A Single Step!</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/June976" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://www.google.com/" target="_blank" rel="noopener" data-popover="Google" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-google"></i></span></a><a class="sidebar-ov-social-item" href="https://weibo.com/" target="_blank" rel="noopener" data-popover="Weibo" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-weibo"></i></span></a><a class="sidebar-ov-social-item" href="https://www.zhihu.com/" target="_blank" rel="noopener" data-popover="Zhihu" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">知</span></a><a class="sidebar-ov-social-item" href="https://blog.csdn.net/HelloWorldTM?type=blog" target="_blank" rel="noopener" data-popover="social.CSDN" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">C</span></a><a class="sidebar-ov-social-item" href="mailto:admin@jun997.xyz" target="_blank" rel="noopener" data-popover="social.QQmail" data-popover-pos="up"><span class="sidebar-ov-social-item__icon">M</span></a></div><div class="sidebar-ov-feed"><span class="sidebar-ov-feed-email"><a class="sidebar-ov-feed-email__link" href="https://mailchi.mp/78fbe8b89a13/juns-blog-subscribe" target="_blank" rel="noopener"><span class="sidebar-ov-feed-email__icon"><i class="fas fa-envelope"></i></span><span>Email Subscribe</span></a></span><span class="sidebar-ov-feed-rss"><a class="sidebar-ov-feed-rss__link" href="/sitemap.xml" target="_blank" rel="noopener"><span class="sidebar-ov-feed-rss__icon"><i class="fas fa-rss"></i></span><span>RSS Subscribe</span></a></span></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">57</div><div class="sidebar-ov-state-item__name">Archives</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">34</div><div class="sidebar-ov-state-item__name">Tags</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span class="sidebar-reading-info__text">You have read </span><span class="sidebar-reading-info__num">0</span><span class="sidebar-reading-info__perc">%</span></div><div class="sidebar-reading-line"></div></div></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2024</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Jun All Rights Reserved</span></div><div><span>Powered by <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a></span><span> v5.4.1</span><span class="footer__devider">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.2</span></div><div class="busuanzi"><span class="busuanzi-siteuv"><span class="busuanzi-siteuv__icon" data-popover-pos="up" data-popover="Unique Visitor"><i class="fas fa-user"></i></span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_uv"></span></span><span class="busuanzi-sitepv"><span class="busuanzi-siteuv__icon" data-popover-pos="up" data-popover="Page View"><i class="fas fa-eye"></i></span><span class="busuanzi-siteuv__value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script src="/js/utils.js?v=2.6.2"></script><script src="/js/stun-boot.js?v=2.6.2"></script><script src="/js/scroll.js?v=2.6.2"></script><script src="/js/header.js?v=2.6.2"></script><script src="/js/sidebar.js?v=2.6.2"></script></body></html>